---
title: "Distributed Link Sparsification for Scalable Scheduling using Graph Neural Networks"
category: 'preprint'
collection: manuscripts
permalink: /publications/2024-02-28-distributed-link-sparsification-for-scalable-scheduling-using-gcn.html
excerpt: 'To reduce the scheduling overhead in wireless networks of dense connectivity, a GCN is employed to adjust contention thresholds for individual links based on traffic statistics and network topology, which is trained with an offline constrained reinforcement learning algorithm capable of balancing two competing objectives.'
date: 2024-02-28
venue: 'IEEE TWC'
paperurl: ''
citation: 'Zhongyuan Zhao, Gunjan Verma, Ananthram Swami, Santiago Segarra, &quot; Distributed Link Sparsification for Scalable Scheduling using Graph Neural Networks,&quot; <i>IEEE Transactions on Wireless Communications</i>, under review'
---

- Related conference paper [Distributed Link Sparsification for Scalable Scheduling using Graph Neural Networks](/publications/2021-10-08-distributed-link-sparsification-for-scalable-scheduling-using-gcn.html)
- Preprint and source code will be available soon


## Abstract

In wireless networks characterized by dense connectivity, the significant signaling overhead generated by distributed link scheduling algorithms can exacerbate issues like congestion, energy consumption, and radio footprint expansion. 
To mitigate these challenges, we propose a distributed link sparsification scheme employing graph neural networks (GNNs) to reduce scheduling overhead for delay-tolerant traffics while maintaining network capacity. 
A GNN module is trained to adjust contention thresholds for individual links based on traffic statistics and network topology, enabling links to withdraw from scheduling contention when unlikely to succeed.
Our approach is facilitated by a novel offline constrained reinforcement learning algorithm capable of balancing two competing objectives: minimizing scheduling overhead while ensuring total utility meets the required level.
In simulated wireless multi-hop networks with up to 500 links, our link sparsification technique effectively alleviates network congestion and reduces radio footprints across four distinct distributed link scheduling protocols.

_Key words_:  Threshold, massive access, scalable  scheduling, graph neural networks, constrained reinforcement learning.

