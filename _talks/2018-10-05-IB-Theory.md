---
title: "On the Information Bottleneck Theory of Deep Learning"
category: 'informal'
collection: talks
type: "Course Project"
permalink: /talks/2018-10-05-IB-Theory.html
venue: "UNL CSCE 990: Deep Learning Seminar"
date: 2018-10-05
location: "Lincoln, Nebraska"
---

This is a presentation of other peoples' papers in course _UNL CSCE 990: Deep Learning Seminar_ lectured by Prof. Stepthen Scott and Prof. Vinodchandran N. Variyam in Fall 2018.

<object data="https://cse.unl.edu/~zhzhao/files/IB_Theory.pdf" type="application/pdf" width="700px" height="400px">
    <embed src="https://cse.unl.edu/~zhzhao/files/IB_Theory.pdf">
        <p>This browser does not support PDFs. Please download the PDF to view it: <a href="https://cse.unl.edu/~zhzhao/files/IB_Theory.pdf">Download PDF</a>.</p>
    </embed>
</object>

[Download Slides Here](https://cse.unl.edu/~zhzhao/files/IB_Theory.pdf)

Related Papers
=====
1. A. M. Saxe, Y. Bansal, J. Dapello, M. Advani, A. Kolchinsky, B.D. Tracey, and D.D. Cox, “On the Information Bottleneck Theory of Deep Learning”, ICLR 2018, [Online] [https://openreview.net/forum?id=ry_WPG-A-](https://openreview.net/forum?id=ry_WPG-A-)
2. R. Schwartz-Ziv and N. Tishby. Opening the black box of deep neural networks via information. arXiv preprint, 2017 [Online][https://arxiv.org/abs/1703.00810](https://arxiv.org/abs/1703.00810)
3. Michal Moshkovich and Naftali Tishby. Mixing complexity and its applications to neural networks. 2017. URL [https://arxiv.org/abs/1703.00729](https://arxiv.org/abs/1703.00729)
4. Naftali Tishby and Noga Zaslavsky. Deep Learning and the information Bottleneck Principle. In Information Theory Workshop (ITW), 2015 IEEE, Pages 1-5. IEEE, 2015
5. Naftali Tishby, Fernando C. Pereira, and William Bialek. The information bottleneck Method. In Proceedings of the 37-th Annual Allerton Conference on Communication, Control and Computing, 1999.



